{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# ------- HYPERPARAMETERS -------\n",
    "IS_LOWERCASE = True  # True if uncased, false otherwise"
   ],
   "metadata": {
    "id": "fXDlaVXuQyGW"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "qgl-XxGpQbbI"
   },
   "outputs": [],
   "source": [
    "# declare word piece tokenizer\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# declare normalizer\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=IS_LOWERCASE)"
   ],
   "metadata": {
    "id": "n396PlFOSy6b"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# declare pre-tokenizer\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ],
   "metadata": {
    "id": "ANlPCpo2S0Xh"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# define trainer\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=30000,\n",
    "                                    min_frequency=2,\n",
    "                                    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "                                    )"
   ],
   "metadata": {
    "id": "4RZXTvJyS1ns"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_lines(files):\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                yield line.strip()\n",
    "\n",
    "\n",
    "files = [\n",
    "    \"/content/train.csv\"\n",
    "]\n",
    "total_lines = sum(1 for file in files for _ in open(file, 'r', encoding='utf-8'))"
   ],
   "metadata": {
    "id": "KEABrAAsbag6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    tqdm(get_lines(files), total=total_lines, desc=\"Training tokenizer\"),\n",
    "    trainer=trainer\n",
    ")"
   ],
   "metadata": {
    "id": "wzMUcXvGS34e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# define post-processor\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "        (\"[PAD]\", tokenizer.token_to_id(\"[PAD]\")),\n",
    "        (\"[UNK]\", tokenizer.token_to_id(\"[UNK]\")),\n",
    "        (\"[MASK]\", tokenizer.token_to_id(\"[MASK]\"))\n",
    "    ],\n",
    ")"
   ],
   "metadata": {
    "id": "y4-Qq0cES6LY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ],
   "metadata": {
    "id": "eReYMya_S7qO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# wrap into a fast tokenizer and save\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "path = f\"/content/drive/MyDrive/tokenizer_{'uncased' if IS_LOWERCASE else 'cased'}\"\n",
    "wrapped_tokenizer.save_pretrained(path)"
   ],
   "metadata": {
    "id": "E74mte9JS-Fa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wrapped_tokenizer.push_to_hub(repo_id=f\"RooseBERT-tokenizer-{'uncased' if IS_LOWERCASE else 'cased'}\")"
   ],
   "metadata": {
    "id": "yPXN3-m-qPNT"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
